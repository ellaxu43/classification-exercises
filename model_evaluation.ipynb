{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5ab5f7f",
   "metadata": {},
   "source": [
    "# Exercises 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "0eb2fca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5af97639",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scenario: Build a classifier to predict whether a given face should unlock the iPhone.\n",
    "\n",
    "#What is the positive and negative case?\n",
    "#postive: face should unlock the iphone. \n",
    "#negative: face shouldn't unlock the iphone. \n",
    "\n",
    "#What are the possible outcomes?\n",
    "#The phone's owner ---->unclocked the phone. \n",
    "#The phone's owner ----> can't unclock the phone. \n",
    "# Somebody else ----->unclocked the phone. \n",
    "# somebody else -----> can't unlock the phone. \n",
    "\n",
    "\n",
    "#What are the costs of the outcomes? The cost of FP is higher than FN. \n",
    "# if anyone can unlock the phone when the cost of fix the phone will occur. \n",
    "# privacy is not protected. \n",
    "\n",
    "#Which metric should we use?\n",
    "#precision.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec8011c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scenario: Predict whether an email is spam or not. Emails marked as spam skip the inbox and go to the spam folder.\n",
    "\n",
    "#What is the positive and negative case?\n",
    "#mark an email is spam\n",
    "#mark an email is not a spam.\n",
    "\n",
    "#What are the possible outcomes?\n",
    "#email is spam ---> spam folder\n",
    "#email is spam ---> main folder \n",
    "#email is not spam ---> spam folder\n",
    "#email is not spam ---> main folder\n",
    "\n",
    "#What are the costs of the outcomes?\n",
    "# The cost of being missing an important email but went to spam. \n",
    "\n",
    "\n",
    "#Which metric should we use?\n",
    "# precision, because missing an important email is more worse. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97190389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario: Predict whether an email is a phishing attempt.\n",
    "#When we predict positive, show an additional banner warning the user \n",
    "#that this might be a phishing email.\n",
    "\n",
    "#What is the positive and negative case?\n",
    "#What are the possible outcomes?\n",
    "#What are the costs of the outcomes?\n",
    "#Which metric should we use?\n",
    "#recall "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67faf0d",
   "metadata": {},
   "source": [
    "## Exercises 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644d3985",
   "metadata": {},
   "source": [
    "## Given the following confusion matrix, evaluate (by hand) the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "476eea42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred dog</th>\n",
       "      <th>pred cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual dog</th>\n",
       "      <td>46</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual cat</th>\n",
       "      <td>13</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            pred dog  pred cat\n",
       "actual dog        46         7\n",
       "actual cat        13        34"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'pred dog': [46, 13],\n",
    "    'pred cat': [7, 34]},\n",
    "    index=['actual dog', 'actual cat']\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af799f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dog = positive class\n",
    "# cat = negative class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93305cb",
   "metadata": {},
   "source": [
    "### In the context of this problem, what is a false positive?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c211b5ae",
   "metadata": {},
   "source": [
    "FN: Predicted dog ---> Actual is not a dog, a cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33070074",
   "metadata": {},
   "source": [
    "### In the context of this problem, what is a false negative?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44111861",
   "metadata": {},
   "source": [
    "FN: Predict is not a dog, a cat ----> Actual is a dog. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83a23ac",
   "metadata": {},
   "source": [
    "### How would you describe this model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386d7c93",
   "metadata": {},
   "source": [
    "TP: predicted dog + actual is dog   46\n",
    "    \n",
    "    \n",
    "FP: predicted dog, but is not dog.  7\n",
    "\n",
    "\n",
    "FN: predicted not dog, but is dog.  13\n",
    "\n",
    "\n",
    "TN: predicted not dog, and is not dog.   34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cba33f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = 46 \n",
    "fp = 7\n",
    "fn = 13\n",
    "tn = 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b140a712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7796610169491526"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall = tp/(tp+fn)\n",
    "recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "373d804f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8679245283018868"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = tp / (tp + fp)\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "40db9577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6508"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "4f908ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred dog</th>\n",
       "      <th>pred cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual dog</th>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual cat</th>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            pred dog  pred cat\n",
       "actual dog        53         0\n",
       "actual cat        47         0"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'pred dog': [53, 47],\n",
    "    'pred cat': [0, 0]},\n",
    "    index=['actual dog', 'actual cat']\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "98753637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp = 53\n",
    "tn = 0\n",
    "fp = 47\n",
    "fn = 0\n",
    "\n",
    "\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bee452d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The model is better than our baseline model so this model is helpful. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394dc8da",
   "metadata": {},
   "source": [
    "## Exercises 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6a59aa",
   "metadata": {},
   "source": [
    "### You are working as a datascientist working for Codeup Cody Creator (C3 for short), a rubber-duck manufacturing plant.\n",
    "\n",
    "Unfortunately, some of the rubber ducks that are produced will have defects. Your team has built several models that try to predict those defects, and the data from their predictions can be found here.\n",
    "\n",
    "Use the predictions dataset and pandas to help answer the following questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "3eecc404",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('c3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "c02677b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      actual     model1  model2     model3\n",
       "0  No Defect  No Defect  Defect  No Defect\n",
       "1  No Defect  No Defect  Defect     Defect"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a470a60",
   "metadata": {},
   "source": [
    "### An internal team wants to investigate the cause of the manufacturing defects. They tell you that they want to identify as many of the ducks that have a defect as possible. Which evaluation metric would be appropriate here? Which model would be the best fit for this use case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9821a23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall -- how often do we get the actual positive cases\n",
    "# recall -- model performance | actual +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "9babaf29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1\n",
      "The recall reate is 50.00%\n",
      "model2\n",
      "The recall reate is 56.25%\n",
      "model3\n",
      "The recall reate is 81.25%\n"
     ]
    }
   ],
   "source": [
    "columns = ['model1','model2','model3']\n",
    "for column in columns: \n",
    "    subset = df[df.actual== 'Defect']\n",
    "    rate =(subset.actual == subset[column]).mean()\n",
    "    print(column)\n",
    "    print(f'The recall reate is {rate:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075cf64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP: predicted defect + actual is defect\n",
    "FP: predicted defect , actual not defect\n",
    "FN: predicted no defect, but is defect \n",
    "TN: predicted no defect, actual no defect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8d531d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5, 0.5625, 0.8125]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recalls = []\n",
    "columns = ['model1','model2','model3']\n",
    "\n",
    "for column in columns: \n",
    "    postive = df[column] == 'Defect'\n",
    "    correct = df.actual == df[column]\n",
    "    tp = df[postive & correct].shape[0] # to get the number of row shape \n",
    "    \n",
    "    wrong = df.actual != df[column]\n",
    "    negative = df[column] != \"Defect\"\n",
    "    fn = df[wrong & negative].shape[0]\n",
    "\n",
    "    \n",
    "    recalls.append(tp/(tp+fn))\n",
    "recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "b2efce06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 3 best fit for recall matrix. 81.25%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237d6712",
   "metadata": {},
   "source": [
    "### Recently several stories in the local news have come out highlighting customers who received a rubber duck with a defect, and portraying C3 in a bad light. The PR team has decided to launch a program that gives customers with a defective duck a vacation to Hawaii. They need you to predict which ducks will have defects, but tell you the really don't want to accidentally give out a vacation package when the duck really doesn't have a defect. Which evaluation metric would be appropriate here? Which model would be the best fit for this use case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7405f85",
   "metadata": {},
   "source": [
    "Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "6fe517bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1\n",
      "model precision: 80.00%\n",
      "-------------\n",
      "model2\n",
      "model precision: 10.00%\n",
      "-------------\n",
      "model3\n",
      "model precision: 13.13%\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "for column in columns: \n",
    "    print(column)\n",
    "    subset = df[df[column]==\"Defect\"]\n",
    "    model = (subset.actual ==subset[column]).mean()\n",
    "    print(f'model precision: {model:.2%}')\n",
    "    print(\"-------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "093c447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 1 best fit for precision matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d307c83f",
   "metadata": {},
   "source": [
    "## Exercises 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e452c3",
   "metadata": {},
   "source": [
    "### You are working as a data scientist for Gives You Paws ™, a subscription based service that shows you cute pictures of dogs or cats (or both for an additional fee).\n",
    "\n",
    "At Gives You Paws, anyone can upload pictures of their cats or dogs. The photos are then put through a two step process. First an automated algorithm tags pictures as either a cat or a dog (Phase I). Next, the photos that have been initially identified are put through another round of review, possibly with some human oversight, before being presented to the users (Phase II).\n",
    "\n",
    "Several models have already been developed with the data, and you can find their results here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "1d209959",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('gives_you_paws.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "a37508ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  actual model1 model2 model3 model4\n",
       "0    cat    cat    dog    cat    dog\n",
       "1    dog    dog    cat    cat    dog"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "adbf9799",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['baseline'] ='dog'\n",
    "df['baselinecat']='cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "75312202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "      <th>baseline</th>\n",
       "      <th>baselinecat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  actual model1 model2 model3 model4 baseline baselinecat\n",
       "0    cat    cat    dog    cat    dog      dog         cat\n",
       "1    dog    dog    cat    cat    dog      dog         cat"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1537fa15",
   "metadata": {},
   "source": [
    "### Model 1 - 4 and baseline accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "0cd8cc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1\n",
      "model accuracy: 80.74%\n",
      "-------------------\n",
      "model2\n",
      "model accuracy: 63.04%\n",
      "-------------------\n",
      "model3\n",
      "model accuracy: 50.96%\n",
      "-------------------\n",
      "model4\n",
      "model accuracy: 74.26%\n",
      "-------------------\n",
      "baseline\n",
      "model accuracy: 65.08%\n",
      "-------------------\n",
      "baselinecat\n",
      "model accuracy: 34.92%\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "accuracy = []\n",
    "columns = ['model1','model2','model3','model4','baseline','baselinecat']\n",
    "\n",
    "for column in columns: \n",
    "    value = (df.actual == df[column]).mean()\n",
    "    accuracy.append(value)\n",
    "    print(column)\n",
    "    print(f'model accuracy: {value:.2%}')\n",
    "    print(\"-------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f9b876",
   "metadata": {},
   "source": [
    " The model 1 have the highest accuracy. 80%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af86336",
   "metadata": {},
   "source": [
    "### Phase 1: model 1 - 4 and recall rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a0e9fe10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "      <th>baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  actual model1 model2 model3 model4 baseline\n",
       "0    cat    cat    dog    cat    dog      dog\n",
       "2    dog    cat    cat    cat    dog      dog"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong = df[df.actual != df.model1]\n",
    "negative = df[df.model1 != 'dog']\n",
    "negative.head(2)\n",
    "#recall = TP/TP+NF\n",
    "#Wrong求的是actual ！= predict\n",
    "#negative 求的是 predict ！= dog\n",
    "# NF 求的是，predict 没有猜狗，但是实际是狗，像下面的2 就会被算进去，但0 不会因为没有猜狗，也不是狗，猜对了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "9f8917a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df[(df.actual != df.model1) & (df.model1 != 'dog')].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "39dc2ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = df[(df.model1 == 'dog') & (df.actual == df.model1)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "3a7f4f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.803318992009834"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b/(a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b0d34df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.803318992009834,\n",
       " 0.49078057775046097,\n",
       " 0.5086047940995697,\n",
       " 0.9557467732022127,\n",
       " 1.0]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recalls = []\n",
    "columns = ['model1','model2','model3','model4','baseline']\n",
    "\n",
    "for column in columns: \n",
    "    postive = df[column] == 'dog'\n",
    "    correct = df.actual == df[column]\n",
    "    tp = df[postive & correct].shape[0] # to get the number of row shape \n",
    "    \n",
    "    wrong = df.actual != df[column]\n",
    "    negative = df[column] != \"dog\"\n",
    "    fn = df[wrong & negative].shape[0]  # here guess cat but actual is dof\n",
    "    recalls.append(tp/(tp+fn))\n",
    "recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "98608590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1\n",
      "The recall reate is 80.33%\n",
      "model2\n",
      "The recall reate is 49.08%\n",
      "model3\n",
      "The recall reate is 50.86%\n",
      "model4\n",
      "The recall reate is 95.57%\n",
      "baseline\n",
      "The recall reate is 100.00%\n"
     ]
    }
   ],
   "source": [
    "columns = ['model1','model2','model3','model4','baseline']\n",
    "for column in columns: \n",
    "    subset = df[df.actual== 'dog']\n",
    "    rate =(subset.actual == subset[column]).mean()\n",
    "    print(column)\n",
    "    print(f'The recall reate is {rate:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a862bee",
   "metadata": {},
   "source": [
    "The model 4 has highest recall rate. 95.57%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd111dda",
   "metadata": {},
   "source": [
    "###  Phase 2: model 1 - 4 and precision rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "e70b0474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1\n",
      "model precision: 89.00%\n",
      "-------------\n",
      "model2\n",
      "model precision: 89.32%\n",
      "-------------\n",
      "model3\n",
      "model precision: 65.99%\n",
      "-------------\n",
      "model4\n",
      "model precision: 73.12%\n",
      "-------------\n",
      "baseline\n",
      "model precision: 65.08%\n",
      "-------------\n",
      "baselinecat\n",
      "model precision: nan%\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "for column in columns: \n",
    "    print(column)\n",
    "    subset = df[df[column]==\"dog\"]\n",
    "    model = (subset.actual ==subset[column]).mean()\n",
    "    print(f'model precision: {model:.2%}')# :2 is save two decimal, % add % after round 2. \n",
    "    #it's better than round, becuase round is only change the display, but :2 and %, change the number. \n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "3145bd6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "      <th>baseline</th>\n",
       "      <th>baselinecat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  actual model1 model2 model3 model4 baseline baselinecat\n",
       "2    dog    cat    cat    cat    dog      dog         cat\n",
       "7    cat    dog    cat    cat    dog      dog         cat"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong = df[df.actual != df.model1] \n",
    "#Pression: TP/ TP+ EP \n",
    "#wrong 求的是predict 的东西和 actual 不一样\n",
    "#postive 求的是predict的是狗，管他predict的对不对，predict 的是狗就算。\n",
    "# EP = 求的是 predict 是狗，但是actual 又不是狗。 7 will be counted \n",
    "\n",
    "\n",
    "#postive = df[column] == 'dog'\n",
    "#correct = df.actual == df[column]\n",
    "#postive 求的是predict  == dog\n",
    "#correct 求的是predict == turth \n",
    "#TP = predict == dog == turth\n",
    "#tp = df[postive & correct].shape[0]\n",
    "\n",
    "wrong.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "07a76472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8900238338440586,\n",
       " 0.8931767337807607,\n",
       " 0.6598883572567783,\n",
       " 0.7312485304490948,\n",
       " 0.6508]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pression = []\n",
    "columns = ['model1','model2','model3','model4','baseline']\n",
    "\n",
    "for column in columns: \n",
    "    postive = df[column] == 'dog'\n",
    "    correct = df.actual == df[column]\n",
    "    tp = df[postive & correct].shape[0] # to get the number of row shape \n",
    "    \n",
    "    wrong = df.actual != df[column]\n",
    "    fp = df[postive & wrong].shape[0]\n",
    "    pression.append(tp/(tp+fp))\n",
    "pression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6e8269",
   "metadata": {},
   "source": [
    "Model 2 has highest pression rate, 89.32%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6194e3b6",
   "metadata": {},
   "source": [
    "### Suppose you are working on a team that solely deals with cat pictures. Which of these models would you recomend for Phase I? For Phase II?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "789d39ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat = postive class\n",
    "# dog = negative class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "4c51808a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "      <th>baseline</th>\n",
       "      <th>baselinecat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  actual model1 model2 model3 model4 baseline baselinecat\n",
       "0    cat    cat    dog    cat    dog      dog         cat\n",
       "1    dog    dog    cat    cat    dog      dog         cat"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "97a9c375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1\n",
      "The recall rate is 81.50%\n",
      "model2\n",
      "The recall rate is 89.06%\n",
      "model3\n",
      "The recall rate is 51.15%\n",
      "model4\n",
      "The recall rate is 34.54%\n",
      "baseline\n",
      "The recall rate is 0.00%\n",
      "baselinecat\n",
      "The recall rate is 100.00%\n"
     ]
    }
   ],
   "source": [
    "columns = ['model1','model2','model3','model4','baseline','baselinecat']\n",
    "for column in columns: \n",
    "    subset = df[df.actual== 'cat']\n",
    "    rate =(subset.actual == subset[column]).mean()\n",
    "    print(column)\n",
    "    print(f'The recall rate is {rate:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "f0eb6921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1\n",
      "model precision: 68.98%\n",
      "-------------\n",
      "model2\n",
      "model precision: 48.41%\n",
      "-------------\n",
      "model3\n",
      "model precision: 35.83%\n",
      "-------------\n",
      "model4\n",
      "model precision: 80.72%\n",
      "-------------\n",
      "baseline\n",
      "model precision: nan%\n",
      "-------------\n",
      "baselinecat\n",
      "model precision: 34.92%\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "for column in columns: \n",
    "    print(column)\n",
    "    subset = df[df[column]==\"cat\"]\n",
    "    model = (subset.actual ==subset[column]).mean()\n",
    "    print(f'model precision: {model:.2%}')# :2 is save two decimal, % add % after round 2. \n",
    "    #it's better than round, becuase round is only change the display, but :2 and %, change the number. \n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bfe0b4",
   "metadata": {},
   "source": [
    "If we only work on cat ( as the postieve) We will use recall as phase I because we don't want to miss out any cat, the best performance is model 2. \n",
    "\n",
    "The phase II we will use precision, the best performance model is also model 4. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafcaf69",
   "metadata": {},
   "source": [
    "## Follow the links below to read the documentation about each function, then apply those functions to the data from the previous problem.\n",
    "\n",
    "sklearn.metrics.accuracy_score\n",
    "\n",
    "sklearn.metrics.precision_score\n",
    "\n",
    "sklearn.metrics.recall_score\n",
    "\n",
    "sklearn.metrics.classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a412d87",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "e53df364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "73cacce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8074, 0.6304, 0.5096, 0.7426, 0.3492]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[accuracy_score(df.actual,df[column]) for column in columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf9d55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = df[df.actual== 'dog']\n",
    "[(subset.actual == subset[column]).mean() for column in columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50da3e84",
   "metadata": {},
   "source": [
    "## Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "c3d5662a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sklearn.metrics as skm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "76e98c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8900238338440586,\n",
       " 0.8931767337807607,\n",
       " 0.6598883572567783,\n",
       " 0.7312485304490948,\n",
       " 0.0]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[skm.precision_score(df.actual,df[column], pos_label='dog') for column in columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d2790f",
   "metadata": {},
   "source": [
    "## Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "533dfd1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.803318992009834,\n",
       " 0.49078057775046097,\n",
       " 0.5086047940995697,\n",
       " 0.9557467732022127,\n",
       " 0.0]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[skm.recall_score(df.actual,df[column], pos_label='dog') for column in columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "5cd423dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.69      0.82      0.75      1746\n",
      "         dog       0.89      0.80      0.84      3254\n",
      "\n",
      "    accuracy                           0.81      5000\n",
      "   macro avg       0.79      0.81      0.80      5000\n",
      "weighted avg       0.82      0.81      0.81      5000\n",
      "\n",
      "model2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.48      0.89      0.63      1746\n",
      "         dog       0.89      0.49      0.63      3254\n",
      "\n",
      "    accuracy                           0.63      5000\n",
      "   macro avg       0.69      0.69      0.63      5000\n",
      "weighted avg       0.75      0.63      0.63      5000\n",
      "\n",
      "model3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.36      0.51      0.42      1746\n",
      "         dog       0.66      0.51      0.57      3254\n",
      "\n",
      "    accuracy                           0.51      5000\n",
      "   macro avg       0.51      0.51      0.50      5000\n",
      "weighted avg       0.55      0.51      0.52      5000\n",
      "\n",
      "model4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.81      0.35      0.48      1746\n",
      "         dog       0.73      0.96      0.83      3254\n",
      "\n",
      "    accuracy                           0.74      5000\n",
      "   macro avg       0.77      0.65      0.66      5000\n",
      "weighted avg       0.76      0.74      0.71      5000\n",
      "\n",
      "baselinecat\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.35      1.00      0.52      1746\n",
      "         dog       0.00      0.00      0.00      3254\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.17      0.50      0.26      5000\n",
      "weighted avg       0.12      0.35      0.18      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for column in columns: \n",
    "    print(column)\n",
    "    print(skm.classification_report(df.actual,df[column]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "f302b1fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.0"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8767d9b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
